## LAB O (Architecture et Fonctionnement d'un Cluster Kubernetes)
![image](https://github.com/user-attachments/assets/d6cabbf4-5fe5-4852-b66d-3e0e98252866)

L'image ci-dessus illustre l'architecture d'un cluster Kubernetes. Dans cet environnement, il existe deux types d'interaction avec le cluster : soit vous √™tes un d√©veloppeur (qui d√©ploie une application ou administre le cluster), soit vous √™tes un utilisateur final (qui consomme une application d√©ploy√©e sur le cluster).

Il y a deux types de n≈ìuds : les n≈ìuds master et les n≈ìuds worker. Les n≈ìuds master abritent les composants n√©cessaires au fonctionnement de Kubernetes, tandis que les n≈ìuds worker poss√®dent leurs propres composants et sont principalement charg√©s d‚Äôex√©cuter les instructions donn√©es par le n≈ìud master.

Concentrons-nous sur le n≈ìud master. Ce dernier, √©galement appel√© n≈ìud gestionnaire, se compose g√©n√©ralement de quatre composants principaux :

1. **API Server (Kube API)**
   - **Description** : Point d'entr√©e principal pour toutes les requ√™tes au cluster Kubernetes. Il expose l'API Kubernetes et g√®re les op√©rations CRUD (Create, Read, Update, Delete) sur les ressources.
   - **R√¥le** : Valide et transforme les demandes en objets de l'API, puis les stocke dans **etcd**, la base de donn√©es cl√©-valeur du cluster.

2. **Kube Controller Manager**
   - **Description** : Ex√©cute les contr√¥leurs qui surveillent l'√©tat des clusters et prennent des mesures pour corriger les d√©s√©quilibres.
   - **R√¥le** : Par exemple, le contr√¥leur de r√©plication veille √† ce qu'un nombre souhait√© de pods soit en cours d'ex√©cution.

3. **Kube Scheduler**
   - **Description** : Prend des d√©cisions sur le n≈ìud sur lequel ex√©cuter les pods non planifi√©s.
   - **R√¥le** : √âvalue les ressources disponibles sur les n≈ìuds, les affinit√©s, les tol√©rances et d'autres contraintes pour effectuer un placement optimal.

4. **Kubelet**
   - **Description** : Agent qui s'ex√©cute sur chaque n≈ìud du cluster, assurant la gestion des pods.
   - **R√¥le** : V√©rifie l'√©tat des conteneurs, les d√©marre, les arr√™te et communique l'√©tat au Kube API Server.

### Exemple d'Interaction entre les Composants

**Lancement d‚Äôun Pod** :
- Lorsqu'une requ√™te de cr√©ation de pod est initi√©e, elle est envoy√©e au Kube API Server, qui valide et enregistre la demande dans etcd.

**Planification du Pod** :
- Le Kube Scheduler surveille les nouvelles demandes de pods, d√©tecte le nouveau pod et √©value les n≈ìuds disponibles pour choisir un n≈ìud appropri√© en fonction des ressources et des contraintes.

**Cr√©ation du Pod sur le N≈ìud** :
- Le Kubelet du n≈ìud s√©lectionn√© re√ßoit l'information concernant le pod √† cr√©er via le Kube API Server.

**Gestion des Conteneurs** :
- Le Kubelet cr√©e le conteneur sur le n≈ìud, le d√©marre et surveille son √©tat.

**Contr√¥le de l'√âtat** :
- Pendant que le pod fonctionne, le Kube Controller Manager surveille l'√©tat des ressources. Si un pod √©choue, il prend des mesures pour le red√©marrer ou le remplacer.


# Lab 1 (Exploration d'un cluster kubernetes)

Sans plus tarder, explorons les composants du cluster en commen√ßant par le n≈ìud principal.

## Commandes √† ex√©cuter

- `kubectl get nodes`
- `kubectl get pods -n kube-system`
- `kubectl get pods -A`
- `kubectl get rs -n kube-system`

```bash
systemctl status kubelet
```
```bash
kubectl delete pod etcd-vmi822295 -n kube-system
```
***Supprimons par exemple etcd (base de donn√©es du cluster). Nous allons nous rendre compte qu'un nouveau pod se cr√©e. Cela est d√ª au fait que les quatre composants du n≈ìud master que nous venons de voir sont des pods statiques.***

## Exploration des Pods statiques
Pour examiner les pods statiques, acc√©dez au r√©pertoire suivant :
```bash
cd /etc/kubernetes/manifests
ls
```

**Ensuite, consultez les fichiers de configuration des composants :**

```bash
cat kube-apiserver.yaml   # Vous pourrez voir les IP de vos services
cat kube-controller-manager.yaml
cat etcd.yaml
```
***Revenons √† kubelet pour v√©rifier son √©tat :***

```bash
systemctl status kubelet
```

***Pour voir la configuration de kubelet, ex√©cutez :***

```bash
cat /etc/kubernetes/kubelet.conf
```
# Lab 2 (Probl√®me de scalabilit√© ou de remont√© en charge sur une application)
Ce fichier permet √† kubelet de communiquer avec Kubernetes, notamment avec l'API server, et contient l'URL de connexion au cluster.
Pour voir la configuration de kubelet, consultez :
```bash
cat /var/lib/kubelet/config.yaml
```
![image](https://github.com/user-attachments/assets/0741f086-96ff-4510-8749-9f046188a5ff)

# Lab 2 (Probl√®me de scalabilit√© ou de mont√©e en charge sur une application ou un microservice)

Supposons que nous avons une application ou un microservice en production qui rencontre des probl√®mes de mont√©e en charge (le microservice est tr√®s sollicit√©, notamment lors de jours de forte affluence, par exemple). Cela peut aussi √™tre d√ª au fait que le nombre d'utilisateurs sollicitant le microservice a doubl√©, voire tripl√©, et que l'application ne r√©pond plus ou met trop de temps √† le faire.

Pour r√©soudre ce genre de souci avec Kubernetes, vous pouvez simplement augmenter le nombre de r√©plicas de votre application, et Kubernetes va √©quilibrer le trafic sur les diff√©rentes instances de votre application.

Appliquez les manifests du dossier `tp-1` et connectez-vous √† l'application.
```bash
cd tp-1
kubectl apply -f .
```
![image](https://github.com/user-attachments/assets/f3d154d6-e2ee-40a7-b31e-d0ac674091a9)
![image](https://github.com/user-attachments/assets/d975fb2b-14bc-4225-8a62-be5680ddb13f)
![image](https://github.com/user-attachments/assets/898e94ec-c7d1-46be-a66e-6af22fd719c0)

Tout se passe comme si vous aviez mis un load balancer devant votre microservice, qui r√©partit la charge sur les diff√©rentes instances de celui-ci.
Bravo ! üéâ

# Lab 3 (d√©ploiement d'une application avec base de donn√©e sur kubernetes)
![image](https://github.com/user-attachments/assets/0a6bb3d2-b94d-4d23-a204-9bf4eebe2247)


Dans ce lab, nous allons d√©ployer l'application Odoo dans un cluster Kubernetes. Odoo est un progiciel de gestion int√©gr√©e de type application √† deux tiers. 

Pour la base de donn√©es, nous utiliserons un service de type ClusterIP, car la base de donn√©es est consomm√©e par l'application frontale, et nous n'avons pas besoin d'y acc√©der depuis l'ext√©rieur. Pour l'application Odoo elle-m√™me (partie frontale), nous l'exposerons via un service de type NodePort, car nous souhaitons pouvoir consommer l'application depuis l'ext√©rieur du cluster.

Pour des raisons sp√©cifiques li√©es aux ressources CPU et m√©moire, nous souhaitons que le microservice de la base de donn√©es ne soit d√©ploy√© que sur le n≈ìud `node01`, car ce n≈ìud dispose des ressources n√©cessaires, ce que `node02` n'a pas. Pour r√©soudre ce genre de contrainte sur Kubernetes, vous pouvez utiliser la notion de taint ou le NodeSelector.


***taint***
![image](https://github.com/user-attachments/assets/0442570e-0863-451f-b771-4f6e7462fc71)
Vous pouvez donc taint vos n≈ìuds et ajouter des tolerations √† vos pods, afin qu'ils ne se d√©ploient que sur des n≈ìuds respectant cette taint. Dans le sch√©ma ci-dessus, le pod (microservice) avec une taint rouge ne pourra se d√©ployer que sur des n≈ìuds avec une taint rouge. Il en va de m√™me pour les pods avec une taint rose et une taint orange.

***affinity***
![image](https://github.com/user-attachments/assets/53fa40e7-a16c-45f2-af1c-a711884be878)

Vous pouvez ajouter un `nodeSelector` dans le manifest de votre microservice, indiquant sur quel n≈ìud il doit se d√©ployer. Cela doit √™tre fait apr√®s avoir √©tiquet√© vos n≈ìuds Kubernetes. 

C'est ce que nous allons faire :

nous allons ajouter un label `app=db` au n≈ìud `node01` et un autre label `front=front-end` au n≈ìud `node02`.
```bash
kubectl label nodes vmi822295 app=db
kubectl label nodes vmi822295 front=front-end
```
pour voir les label sur un noeud
```bash
kubectl get node vmi822295 --show-labels
```
pour supprimer le label **app** sur le node **vmi822295**
```bash
kubectl label nodes vmi822295 app-
```

Ainsi, en utilisant ce code dans notre microservice, il ne pourra se d√©ployer que sur le n≈ìud √©tiquet√© `app=db`.
**voir le les manifest(dossier odoo et postgres)**

![image](https://github.com/user-attachments/assets/f6f87240-54c2-410b-a3c7-5a1c74a55d95)

![image](https://github.com/user-attachments/assets/45c13ef5-39c6-4bda-93c9-33335e1280cd)

***d√©ploiement**
```bash
cd postgres
kubectl apply -f .
kubectl get all -n ic-webapp
```
![image](https://github.com/user-attachments/assets/2acea960-6fba-4045-ba80-3f56a90158d1)

```bash
cd ../odoo
kubectl apply -f .
kubectl get all -n ic-webapp
```
![image](https://github.com/user-attachments/assets/5321cd2d-e867-4fba-882f-1abc36dfc481)
Bravo ! üéâ
Bravo ! üéâ
Bravo ! üéâ
# Lab 4 : Backup du Cluster

Nous venons d'installer nos microservices sur le cluster. Supposons qu'un incident survienne et que nous souhaitions revenir en arri√®re. Bien que nos manifests soient sauvegard√©s sur un d√©p√¥t distant, il peut √™tre n√©cessaire de restaurer l'√©tat pr√©c√©dent du cluster. Il est donc essentiel de r√©aliser des sauvegardes, notamment de l'etcd (la base de donn√©es du cluster). Pour ce faire, nous allons utiliser la ligne de commande `etcdctl`. 

## √âtapes d'installation

1. **R√©cup√©ration de la version de votre etcd :**

   Ex√©cutez la commande suivante pour obtenir la version d'etcd install√©e sur votre cluster :

```bash
   kubectl -n kube-system get pod/etcd-vmi822295 -o=jsonpath='{$.spec.containers[:1].image}'
```
   Cette commande vous donnera la version d'etcd install√©e sur votre cluster. Dans notre cas, nous avons etcd:3.5.4, comme le montre le sch√©ma ci-dessous : ![image](https://github.com/user-attachments/assets/c569c7d6-d6c8-4af2-bf34-012eb62f04dd)
.

2. **Installation d'etcdctl : Voici les commandes √† ex√©cuter**

```bash
# D√©but de l'installation d'etcd
ETCD_VER=v3.5.4

# Choisissez l'URL
GOOGLE_URL=https://storage.googleapis.com/etcd
GITHUB_URL=https://github.com/etcd-io/etcd/releases/download
DOWNLOAD_URL=${GOOGLE_URL}

rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
rm -rf /tmp/etcd-download-test && mkdir -p /tmp/etcd-download-test

curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz
tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1
rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz

/tmp/etcd-download-test/etcd --version
/tmp/etcd-download-test/etcdctl version

ln -s /tmp/etcd-download-test/etcdctl /usr/bin/etcdctl
etcdctl version
# Fin de l'installation d'etcd
```
![image](https://github.com/user-attachments/assets/a9696941-0e19-4efa-8794-29df17699b8a)

# Backup du Cluster

Pour voir la configuration et comprendre comment `kube-apiserver` communique avec les autres composants, vous pouvez ex√©cuter la commande suivante :

```bash
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep etcd
```

```bash
rm /tmp/backup.db

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
snapshot save /tmp/backup.db
```
**vous pouvez voir la taile de votre base de donn√©es kubernetes**

```bash
ETCDCTL_API=3 etcdctl --write-out=table snapshot status /tmp/backup.db
```
![image](https://github.com/user-attachments/assets/ff1ae35f-824e-4c26-b4ff-d69626b03505)


Nous allons maintenant simuler une incident sur le cluster et effectuer une restauration. Pour ce faire, rendez-vous dans le dossier d'Odoo, puis supprimez les pods avec la commande suivante :

```bash
kubectl delete -f .
```
![image](https://github.com/user-attachments/assets/72111042-5c96-43a0-a82a-4e0b4f8576b1)

Pour commencer la restauration, nous allons d'abord supprimer les pods statiques afin d'√©viter d'√©crire dans la base de donn√©es pendant cette phase de restauration. Pour ce faire, ex√©cutez les commandes suivantes :

```bash
mv /etc/kubernetes/manifests/kube-apiserver.yaml ./
mv /etc/kubernetes/manifests/etcd.yaml ./
rm -rf /var/lib/etcd
```

Nous pouvons donc restaurer notre cluster en utilisant la commande suivante :

```bash
ETCDCTL_API=3 etcdctl --data-dir="/var/lib/etcd" snapshot restore /tmp/backup.db
```
une fois la restoration nous pouvons remetre les fichiers static
```bash
mv kube-apiserver.yaml /etc/kubernetes/manifests/
mv etcd.yaml /etc/kubernetes/manifests/
```
![image](https://github.com/user-attachments/assets/d669038b-1675-4e0e-bd39-62803a2b9073)

v√©rifions la disponibilit√© de notre application
![image](https://github.com/user-attachments/assets/12ff1bd3-d946-42e6-a8b6-a32b9f643257)
Bravo ! üéâ
Bravo ! üéâ

***NB: En production, vous auriez besoin d'un outil plus complet comme Velero pour effectuer les sauvegardes de vos applications, d'un microservice en particulier ou d'un volume persistant sp√©cifiquement. Vous pouvez √©galement sauvegarder toutes les applications d'un namespace.**
![image](https://github.com/user-attachments/assets/7a5459cb-716e-4c10-a57a-b09b4e2899cc)
